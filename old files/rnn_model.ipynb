{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mid-cap funds can deliver more, stay put: Experts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mid caps now turn into market darlings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hudco raises Rs 279 cr via tax-free bonds</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXL beats profit estimates, cuts sales outlook</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would stick to banking: Girish Pai, Centrum Br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  Mid-cap funds can deliver more, stay put: Experts          1\n",
       "1             Mid caps now turn into market darlings          1\n",
       "2          Hudco raises Rs 279 cr via tax-free bonds          1\n",
       "3     EXL beats profit estimates, cuts sales outlook          1\n",
       "4  Would stick to banking: Girish Pai, Centrum Br...          1"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('Cleaned_data/combined_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9761\n",
       "0    6201\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample the Dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6201\n",
       "0    6201\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority (class 1) and minority (class 0) samples\n",
    "df_majority = df[df['Sentiment'] == 1]\n",
    "df_minority = df[df['Sentiment'] == 0]\n",
    "\n",
    "# Downsample the majority class to match the count of the minority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,  \n",
    "                                   n_samples=len(df_minority),  \n",
    "                                   random_state=42)  \n",
    "\n",
    "# Combine the downsampled majority class with the original minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_downsampled['Sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/x65fp4pn31s5cv4gsbh_3cl80000gn/T/ipykernel_66158/378033684.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_downsampled['Sentence'] = df_downsampled['Sentence'].str.replace(char,\"\")\n"
     ]
    }
   ],
   "source": [
    "# removing non-alphanumeric characters since not needed\n",
    "\n",
    "non_alphanum = [',','.','/','\"',':',';','!','$', '%', '@','#',\"'\",\"*\",\"(\",\")\",\"&\",\"--\"]\n",
    "for char in non_alphanum:\n",
    "  df_downsampled['Sentence'] = df_downsampled['Sentence'].str.replace(char,\"\")\n",
    "\n",
    "df_downsampled['Sentence'] = df_downsampled['Sentence'].str.replace(\" s \",\" \")\n",
    "df_downsampled['Sentence'] = df_downsampled['Sentence'].str.replace(\" '\",\"'\")\n",
    "df_downsampled['Sentence'] = df_downsampled['Sentence'].str.replace(\"  \",\" \")\n",
    "df_downsampled['Sentence'] = df_downsampled['Sentence'].str.replace(\"   \",\" \")\n",
    "df_downsampled['Sentence'] = df_downsampled['Sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_downsampled['Sentence'].to_numpy().reshape(-1, 1)\n",
    "y = df_downsampled['Sentiment'].to_numpy().reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting each sentence and sentiment from the training and testing dataframes into lists\n",
    "\n",
    "training_sentences=[]\n",
    "testing_sentences=[]\n",
    "training_labels=[]\n",
    "testing_labels=[]\n",
    "\n",
    "for i in X_train:\n",
    "  training_sentences.append(i[0])\n",
    "for i in y_train:\n",
    "  training_labels.append(i[0])\n",
    "for i in X_test:\n",
    "  testing_sentences.append(i[0])\n",
    "for i in y_test:\n",
    "  testing_labels.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "vocab_size = 10000  # Size of vocabulary\n",
    "embedding_dim = 100  # Dimension of word embeddings\n",
    "max_length = 300\n",
    "sequence_length = 300  # Length of input sequences\n",
    "num_classes = 2  # Number of sentiment classes (e.g., positive and negative)\n",
    "trunc_type='post'\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting tokenizer to training sentences\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# padding\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a feed forward nn model\n",
    "\n",
    "# model =  tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "# model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "# model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a RNN model using GRU\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add an Embedding layer for word embeddings\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length))\n",
    "\n",
    "# # Add GRU layer\n",
    "# model.add(GRU(64, return_sequences=True))  \n",
    "# model.add(GRU(64))  \n",
    "\n",
    "# # Add a Dense layer for classification\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_17 (Embedding)    (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 300, 64)           42240     \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1075329 (4.10 MB)\n",
      "Trainable params: 1075329 (4.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a RNN model using LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer for word embeddings\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length))\n",
    "\n",
    "# Add LSTM layer \n",
    "model.add(LSTM(64, return_sequences=True))  \n",
    "model.add(LSTM(64))  \n",
    "\n",
    "# Add a Dense layer for classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "291/291 - 45s - loss: 0.6934 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.5002 - 45s/epoch - 155ms/step\n",
      "Epoch 2/10\n",
      "291/291 - 44s - loss: 0.6932 - accuracy: 0.5038 - val_loss: 0.6931 - val_accuracy: 0.5002 - 44s/epoch - 151ms/step\n",
      "Epoch 3/10\n",
      "291/291 - 44s - loss: 0.6933 - accuracy: 0.4969 - val_loss: 0.6932 - val_accuracy: 0.5002 - 44s/epoch - 150ms/step\n",
      "Epoch 4/10\n",
      "291/291 - 44s - loss: 0.6933 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.5002 - 44s/epoch - 150ms/step\n",
      "Epoch 5/10\n",
      "291/291 - 44s - loss: 0.6933 - accuracy: 0.4836 - val_loss: 0.6931 - val_accuracy: 0.5002 - 44s/epoch - 150ms/step\n",
      "Epoch 6/10\n",
      "291/291 - 44s - loss: 0.6932 - accuracy: 0.4945 - val_loss: 0.6932 - val_accuracy: 0.4998 - 44s/epoch - 150ms/step\n",
      "Epoch 7/10\n",
      "291/291 - 44s - loss: 0.6932 - accuracy: 0.4949 - val_loss: 0.6931 - val_accuracy: 0.4998 - 44s/epoch - 150ms/step\n",
      "Epoch 8/10\n",
      "291/291 - 44s - loss: 0.6932 - accuracy: 0.4965 - val_loss: 0.6931 - val_accuracy: 0.5002 - 44s/epoch - 150ms/step\n",
      "Epoch 9/10\n",
      "291/291 - 44s - loss: 0.6932 - accuracy: 0.4907 - val_loss: 0.6931 - val_accuracy: 0.5002 - 44s/epoch - 150ms/step\n",
      "Epoch 10/10\n",
      "291/291 - 44s - loss: 0.6932 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4998 - 44s/epoch - 153ms/step\n"
     ]
    }
   ],
   "source": [
    "# fitting model\n",
    "\n",
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
